{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODTLCYWWE9MlwQDwDyZfUz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedgomaagoda/credit-fraud-detection/blob/main/CreditFraudDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cEKJLdm198l"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import svm\n",
        "import itertools\n",
        "# Matplotlib library to plot the charts\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "# Library for the statistic data vizualisation\n",
        "import seaborn\n",
        "data = pd.read_csv('../input/creditcard.csv') # Reading the file .csv\n",
        "df = pd.DataFrame(data) # Converting data to Panda DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "df.describe()\n",
        "df_fraud = df[df['Class'] == 1] # Recovery of fraud data\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\n",
        "plt.title('Scratter plot amount fraud')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Amount')\n",
        "plt.xlim([0,175000])\n",
        "plt.ylim([0,2500])\n",
        "plt.show()\n",
        "nb_big_fraud = df_fraud[df_fraud['Amount'] > 1000].shape[0] # Recovery of frauds over 1000\n",
        "print('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')\n",
        "number_fraud = len(data[data.Class == 1])\n",
        "number_no_fraud = len(data[data.Class == 0])\n",
        "print('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')\n",
        "df_corr = df.corr()\n",
        "plt.figure(figsize=(15,10))\n",
        "seaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\n",
        "seaborn.set(font_scale=2,style='white')\n",
        "plt.title('Heatmap correlation')\n",
        "plt.show()\n",
        "rank = df_corr['Class'] # Retrieving the correlation coefficients per feature in relation to the feature class\n",
        "df_rank = pd.DataFrame(rank)\n",
        "df_rank = np.abs(df_rank).sort_values(by='Class',ascending=False)\n",
        "df_rank.dropna(inplace=True)\n",
        "df_train_all = df[0:150000] # We cut in two the original dataset\n",
        "df_train_1 = df_train_all[df_train_all['Class'] == 1] # We seperate the data which are the frauds and the no frauds\n",
        "df_train_0 = df_train_all[df_train_all['Class'] == 0]\n",
        "print('In this dataset, we have ' + str(len(df_train_1)) +\" frauds so we need to take a similar number of non-fraud\")\n",
        "\n",
        "df_sample=df_train_0.sample(300)\n",
        "df_train = df_train_1.append(df_sample) # We gather the frauds with the no frauds.\n",
        "df_train = df_train.sample(frac=1)\n",
        "X_train = df_train.drop(['Time', 'Class'],axis=1) # We drop the features Time (useless), and the Class (label)\n",
        "y_train = df_train['Class'] # We create our label\n",
        "X_train = np.asarray(X_train)\n",
        "y_train = np.asarray(y_train)\n",
        "df_test_all = df[150000:]\n",
        "\n",
        "X_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\n",
        "y_test_all = df_test_all['Class']\n",
        "X_test_all = np.asarray(X_test_all)\n",
        "y_test_all = np.asarray(y_test_all)\n",
        "X_train_rank = df_train[df_rank.index[1:11]] # We take the first ten ranked features\n",
        "X_train_rank = np.asarray(X_train_rank)\n",
        "X_test_all_rank = df_test_all[df_rank.index[1:11]]\n",
        "X_test_all_rank = np.asarray(X_test_all_rank)\n",
        "y_test_all = np.asarray(y_test_all)\n",
        "classifier = svm.SVC(kernel='linear')\n",
        "classifier.fit(X_train, y_train)\n",
        "prediction_SVM_all = classifier.predict(X_test_all)\n",
        "cm = confusion_matrix(y_test_all, prediction_SVM_all)\n",
        "plot_confusion_matrix(cm,class_names)\n",
        "print('Our criterion give a result of '\n",
        "      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n",
        "print('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\n",
        "print('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\n",
        "print(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n",
        "classifier.fit(X_train_rank, y_train) # Then we train our model, with our balanced data train.\n",
        "prediction_SVM = classifier.predict(X_test_all_rank)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "mhzpqaffre2-"
      }
    }
  ]
}